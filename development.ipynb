{
 "metadata": {
  "name": "",
  "signature": "sha256:dc940c908c967e7ea8fa74f1d2fa189b361ca72ebe922a8970e4e2f6b3e9f6bf"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sqlalchemy import create_engine\n",
      "import tweepy as tweepy\n",
      "import pandas as pd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from tweet_functions import *\n",
      "from credentials import *"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.stem.snowball import SnowballStemmer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "db_connection_string = \"mysql+pymysql://{}:{}@{}:{}/{}?charset=utf8mb4&use_unicode=True\".format(DB_USER, DB_PASS, DB_HOST, DB_PORT, DB_SCHEMA)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "db_engine = create_engine(db_connection_string, encoding=\"utf8\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "auth = tweepy.OAuthHandler(TWITTER_CONSUMER_TOKEN, TWITTER_CONSUMER_SECRET)\n",
      "auth.set_access_token(TWITTER_ACCESS_TOKEN, TWITTER_ACCESS_TOKEN_SECRET)\n",
      "api = tweepy.API(auth)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# log the start of thebatch\n",
      "connection = db_engine.raw_connection()\n",
      "try:\n",
      "    cursor = connection.cursor()\n",
      "    results = cursor.callproc('start_batch')\n",
      "    results = list(cursor.fetchall())\n",
      "    cursor.close()\n",
      "    connection.commit()\n",
      "finally:\n",
      "    connection.close()\n",
      "batch_id = results[0][0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "term_df = pd.read_sql('call get_terms()', db_engine)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "max_tweets_per_term = 1000\n",
      "stemmer = SnowballStemmer(\"english\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for index, term_row in term_df.iterrows():\n",
      "    term_id = term_row['term_id']\n",
      "    term = term_row['term']\n",
      "    term_max_id = term_row['max_id']\n",
      "    \n",
      "    searched_tweets = []\n",
      "    last_id = -1\n",
      "    \n",
      "    while len(searched_tweets) < max_tweets_per_term:\n",
      "        count = max_tweets_per_term - len(searched_tweets)\n",
      "        try:\n",
      "            new_tweets = api.search(q=term, count=count, max_id=str(last_id - 1), since_id=term_max_id)\n",
      "            if not new_tweets:\n",
      "                break\n",
      "            searched_tweets.extend(new_tweets)\n",
      "            last_id = new_tweets[-1].id\n",
      "        except tweepy.TweepError as e:\n",
      "            # depending on TweepError.code, one may want to retry or wait\n",
      "            # to keep things simple, we will give up on an error\n",
      "            break\n",
      "    \n",
      "    number_collected = len(searched_tweets)\n",
      "    if number_collected > 0:\n",
      "        # there were tweets... so need to format and upload them to DB\n",
      "        \n",
      "        # first quickly get the new max id and save it \n",
      "        new_max_id = str(searched_tweets[0].id)    \n",
      "\n",
      "        # turn the collection of Status objects into a dataframe\n",
      "        tweets, headers = tweets_to_list(searched_tweets)\n",
      "        tweets_df = pd.DataFrame(tweets, columns=headers)\n",
      "\n",
      "        # make a smaller data frame of just the text, then remove any urls from the text:\n",
      "        tweet_text_df = tweets_df[['tweet_id','tweet_text']]\n",
      "        tweet_text_df.loc[:,'tweet_text'] = tweet_text_df['tweet_text'].apply(removeURL)\n",
      "        # could also remove other stuff here... maybe @name tags?\n",
      "        \n",
      "        # apply tokenizer function to get message_id <-> word table, then stem those words\n",
      "        words_df = tweet_text_df.groupby('tweet_id', group_keys=False).apply(applyWords)\n",
      "        words_df.loc[:,'word'] = words_df['word'].apply(stemmer.stem)\n",
      "\n",
      "        # going back to the Status objects, now make a dataframe of entities\n",
      "        entities, headers = tweets_to_entity_list(searched_tweets)\n",
      "        entities_df = pd.DataFrame(entities, columns=headers)\n",
      "        \n",
      "        # add the term ID to tweets df \n",
      "        tweets_df['term_id'] = term_id\n",
      "        \n",
      "        try:\n",
      "            connection = db_engine.raw_connection()\n",
      "            cursor = connection.cursor()\n",
      "            \n",
      "            try:\n",
      "                cursor.callproc('truncate_staging')\n",
      "            except:\n",
      "                print(\"Error truncating staging\")\n",
      "            \n",
      "            try:\n",
      "                tweets_df.to_sql('stage_tweet', db_engine, if_exists='append', index=False)\n",
      "                cursor.callproc('merge_stage_tweet', [str(batch_id)])\n",
      "            except:\n",
      "                print(\"Error loading/merging tweets\")\n",
      "            \n",
      "            try:\n",
      "                words_df.to_sql('stage_word', db_engine, if_exists='append', index=False)\n",
      "                cursor.callproc('merge_stage_word')\n",
      "            except:\n",
      "                print(\"Error loading/merging words\")\n",
      "            \n",
      "            try:\n",
      "                entities_df.to_sql('stage_entity', db_engine, if_exists='append', index=False)\n",
      "                cursor.callproc('merge_stage_entity')\n",
      "            except:\n",
      "                print(\"Error loading/merging entities\")            \n",
      "                \n",
      "            cursor.close()\n",
      "            connection.commit()\n",
      "        finally:\n",
      "            connection.close()\n",
      "    \n",
      "    else:\n",
      "        # no tweets, so ignore the stuff above, and set the max_id to same as before\n",
      "        new_max_id = term_max_id\n",
      "    \n",
      "    # tweets or no, log the term batch before moving on to next term\n",
      "    connection = db_engine.raw_connection()\n",
      "    try:\n",
      "        cursor = connection.cursor()\n",
      "        cursor.callproc('log_batch_term', [str(batch_id), str(term_id), new_max_id, str(number_collected)])\n",
      "        cursor.close()\n",
      "        connection.commit()\n",
      "    finally:\n",
      "        connection.close()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}