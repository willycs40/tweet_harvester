{
 "metadata": {
  "name": "",
  "signature": "sha256:a3e0d44e3dab2f31400b345eb3ac695def650cc458b0b0b0298ab984908acd44"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sqlalchemy import create_engine\n",
      "import tweepy as tweepy\n",
      "import pandas as pd\n",
      "import HTMLParser\n",
      "import re"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tokenize import word_tokenize\n",
      "from nltk.tokenize import RegexpTokenizer\n",
      "from nltk.stem.snowball import SnowballStemmer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "db_engine = create_engine(\"mysql+pymysql://user:pass@ip_address/tweet_harvester?charset=utf8mb4&use_unicode=True\", encoding=\"utf8\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "consumer_token = ''\n",
      "consumer_secret=''\n",
      "access_token = ''\n",
      "access_token_secret = ''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "auth = tweepy.OAuthHandler(consumer_token, consumer_secret)\n",
      "auth.set_access_token(access_token, access_token_secret)\n",
      "api = tweepy.API(auth)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# log the start of thebatch\n",
      "connection = db_engine.raw_connection()\n",
      "try:\n",
      "    cursor = connection.cursor()\n",
      "    results = cursor.callproc('start_batch')\n",
      "    results = list(cursor.fetchall())\n",
      "    cursor.close()\n",
      "    connection.commit()\n",
      "finally:\n",
      "    connection.close()\n",
      "batch_id = results[0][0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "term_df = pd.read_sql('call get_terms()', db_engine)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "max_tweets_per_term = 1000\n",
      "stemmer = SnowballStemmer(\"english\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def tweets_to_list(searched_tweets):\n",
      "    h = HTMLParser.HTMLParser()\n",
      "    formatted_tweets = []\n",
      "\n",
      "    for t in searched_tweets:\n",
      "        t_list = [t.id, t.created_at, t.lang, t.in_reply_to_status_id, t.retweet_count, t.favorite_count, t.user.location, t.user.friends_count, t.user.followers_count, t.user.statuses_count]\n",
      "\n",
      "        if t.text is not None:\n",
      "            t_list.extend([h.unescape(t.text)])\n",
      "        else:\n",
      "            t_list.extend([None])\n",
      "\n",
      "        if t.user.screen_name is not None:\n",
      "            t_list.extend([h.unescape(t.user.screen_name)])\n",
      "        else:\n",
      "            t_list.extend([None])\n",
      "\n",
      "        if t.in_reply_to_screen_name is not None:\n",
      "            t_list.extend([h.unescape(t.in_reply_to_screen_name)])\n",
      "        else:\n",
      "            t_list.extend([None])\n",
      "\n",
      "        if t.coordinates is not None and t.coordinates['type']=='Point' and t.coordinates['coordinates'][0]<>0:\n",
      "            t_list.extend([t.coordinates['coordinates'][0], t.coordinates['coordinates'][1]])\n",
      "\n",
      "        else:\n",
      "            t_list.extend([None,None])\n",
      "\n",
      "        if hasattr(t, 'retweeted_status'):\n",
      "            t_list.extend([t.retweeted_status.id, t.retweeted_status.user.screen_name])\n",
      "        else:\n",
      "            t_list.extend([None, None])\n",
      "\n",
      "        formatted_tweets.append(t_list)\n",
      "        \n",
      "    column_list = ['tweet_id','created_at', 'language', 'in_reply_to_status_id', 'retweet_count', 'favorite_count', 'user_location', 'user_friends_count', 'user_followers_count', 'user_statuses_count', 'tweet_text', 'user_screen_name', 'in_reply_to_screen_name', 'longitude', 'latitude', 'retweeted_status_id', 'retweeted_screen_name']\n",
      "\n",
      "    return [formatted_tweets, column_list]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def tweets_to_entity_list(searched_tweets):\n",
      "    entities_list = []\n",
      "    \n",
      "    for tweet in searched_tweets:\n",
      "        if 'user_mentions' in tweet.entities:\n",
      "            for mention in tweet.entities['user_mentions']:\n",
      "                entities_list.append([tweet.id, 'mention', mention['screen_name']])\n",
      "\n",
      "        if 'urls' in tweet.entities:\n",
      "            for url in tweet.entities['urls']:\n",
      "                 entities_list.append([tweet.id, 'url', url['expanded_url']])\n",
      "\n",
      "        if 'media' in tweet.entities:\n",
      "            for med in tweet.entities['media']:\n",
      "                 entities_list.append([tweet.id,med['type'],med['media_url']])\n",
      "\n",
      "        if 'hashtags' in tweet.entities:\n",
      "            for tag in tweet.entities['hashtags']:\n",
      "                 entities_list.append([tweet.id,'hashtag',tag['text']])\n",
      "        \n",
      "    column_list = ['tweet_id', 'type', 'value']\n",
      "    \n",
      "    return entities_list, column_list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def removeURL(content_string):\n",
      "    # compile a regex to find urls\n",
      "    url_regex = re.compile(r\"(?P<url>https?://[^\\s]+)\")\n",
      "\n",
      "    # replace any urls with blank\n",
      "    return url_regex.sub(\"\",content_string)\n",
      "\n",
      "def applyWords(group):\n",
      "    # define tokenizer to break strings into words (allow hashtags, a-Z0-9)\n",
      "    # todo: prevent number-only words being tokenized\n",
      "    tokenizer = RegexpTokenizer(r'#?\\w+')\n",
      "    # for each row, return a row per token, with sequence number\n",
      "\n",
      "    row = group.irow(0)\n",
      "    wordlist = tokenizer.tokenize(row[1])\n",
      "    return pd.DataFrame({'word':wordlist, 'sequence': range(1,len(wordlist)+1), 'tweet_id':row[0]})\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for index, term_row in term_df.iterrows():\n",
      "    term_id = term_row['term_id']\n",
      "    term = term_row['term']\n",
      "    term_max_id = term_row['max_id']\n",
      "    \n",
      "    searched_tweets = []\n",
      "    last_id = -1\n",
      "    \n",
      "    while len(searched_tweets) < max_tweets_per_term:\n",
      "        count = max_tweets_per_term - len(searched_tweets)\n",
      "        try:\n",
      "            new_tweets = api.search(q=term, count=count, max_id=str(last_id - 1), since_id=term_max_id)\n",
      "            if not new_tweets:\n",
      "                break\n",
      "            searched_tweets.extend(new_tweets)\n",
      "            last_id = new_tweets[-1].id\n",
      "        except tweepy.TweepError as e:\n",
      "            # depending on TweepError.code, one may want to retry or wait\n",
      "            # to keep things simple, we will give up on an error\n",
      "            break\n",
      "    \n",
      "    number_collected = len(searched_tweets)\n",
      "    if number_collected > 0:\n",
      "        # there were tweets... so need to format and upload them to DB\n",
      "        \n",
      "        # first quickly get the new max id and save it \n",
      "        new_max_id = str(searched_tweets[0].id)    \n",
      "\n",
      "        # turn the collection of Status objects into a dataframe\n",
      "        tweets, headers = tweets_to_list(searched_tweets)\n",
      "        tweets_df = pd.DataFrame(tweets, columns=headers)\n",
      "\n",
      "        # make a smaller data frame of just the text, then remove any urls from the text:\n",
      "        tweet_text_df = tweets_df[['tweet_id','tweet_text']]\n",
      "        tweet_text_df.loc[:,'tweet_text'] = tweet_text_df['tweet_text'].apply(removeURL)\n",
      "        # could also remove other stuff here... maybe @name tags?\n",
      "        \n",
      "        # apply tokenizer function to get message_id <-> word table, then stem those words\n",
      "        words_df = tweet_text_df.groupby('tweet_id', group_keys=False).apply(applyWords)\n",
      "        words_df.loc[:,'word'] = words_df['word'].apply(stemmer.stem)\n",
      "\n",
      "        # going back to the Status objects, now make a dataframe of entities\n",
      "        entities, headers = tweets_to_entity_list(searched_tweets)\n",
      "        entities_df = pd.DataFrame(entities, columns=headers)\n",
      "        \n",
      "        # add the term ID to tweets df \n",
      "        tweets_df['term_id'] = term_id\n",
      "        \n",
      "        try:\n",
      "            connection = db_engine.raw_connection()\n",
      "            cursor = connection.cursor()\n",
      "            \n",
      "            try:\n",
      "                cursor.callproc('truncate_staging')\n",
      "            except:\n",
      "                print(\"Error truncating staging\")\n",
      "            \n",
      "            try:\n",
      "                tweets_df.to_sql('stage_tweet', db_engine, if_exists='append', index=False)\n",
      "                cursor.callproc('merge_stage_tweet', [str(batch_id)])\n",
      "            except:\n",
      "                print(\"Error loading/merging tweets\")\n",
      "            \n",
      "            try:\n",
      "                words_df.to_sql('stage_word', db_engine, if_exists='append', index=False)\n",
      "                cursor.callproc('merge_stage_word')\n",
      "            except:\n",
      "                print(\"Error loading/merging words\")\n",
      "            \n",
      "            try:\n",
      "                entities_df.to_sql('stage_entity', db_engine, if_exists='append', index=False)\n",
      "                cursor.callproc('merge_stage_entity')\n",
      "            except:\n",
      "                print(\"Error loading/merging entities\")            \n",
      "                \n",
      "            cursor.close()\n",
      "            connection.commit()\n",
      "        finally:\n",
      "            connection.close()\n",
      "    \n",
      "    else:\n",
      "        # no tweets, so ignore the stuff above, and set the max_id to same as before\n",
      "        new_max_id = term_max_id\n",
      "    \n",
      "    # tweets or no, log the term batch before moving on to next term\n",
      "    connection = db_engine.raw_connection()\n",
      "    try:\n",
      "        cursor = connection.cursor()\n",
      "        cursor.callproc('log_batch_term', [str(batch_id), str(term_id), new_max_id, str(number_collected)])\n",
      "        cursor.close()\n",
      "        connection.commit()\n",
      "    finally:\n",
      "        connection.close()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "C:\\Users\\will\\Anaconda\\lib\\site-packages\\pandas\\core\\indexing.py:415: SettingWithCopyWarning: \n",
        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
        "  self.obj[item] = s\n"
       ]
      }
     ],
     "prompt_number": 13
    }
   ],
   "metadata": {}
  }
 ]
}